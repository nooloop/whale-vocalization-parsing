{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5953990",
   "metadata": {},
   "source": [
    "# Whale Vocalization Parsing\n",
    "\n",
    "This notebook contains the following functionalities:\n",
    "\n",
    "- Visualizing Data (time- and frequency-domain representations) \n",
    "- Dynamic time warping between two signals\n",
    "- Cross correlation between two signals\n",
    "- Monophonic pitch detection (zero-crossing, FFT peak, autocorrelation)\n",
    "- Signal denoising (on a per signal basis)\n",
    "- Locating probe signal in query for n-gram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell loads packages:\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import sklean\n",
    "import scipy\n",
    "import bokeh\n",
    "import librosa\n",
    "import fastdtw\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32925938",
   "metadata": {},
   "source": [
    "# Audio Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell loads audio:\n",
    "\n",
    "def load_audio(filename):\n",
    "    \n",
    "    # read in file and convert to range [-1, 1]:\n",
    "    \n",
    "    srate, audio = scipy.io.wavfile.read(filename)\n",
    "    audio = audio.astype(np.float32) / 32767.0 \n",
    "    \n",
    "    # set max to 0.9:\n",
    "    \n",
    "    if (len(audio.shape) == 1): \n",
    "        audio = (0.9 / max(np.abs(audio)) * audio)\n",
    "    else: \n",
    "        audio[:,0] = (0.9 / max(np.abs(audio[:,0])) * audio[:,0])\n",
    "        audio[:,1] = (0.9 / max(np.abs(audio[:,1])) * audio[:,1])\n",
    "        return audio.transpose(), srate\n",
    "    \n",
    "    # return audio:\n",
    "    \n",
    "    return audio, srate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6f34f",
   "metadata": {},
   "source": [
    "# Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccecfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the time-domain representation of an audio file:\n",
    "\n",
    "def plot_time_domain(audio, srate): \n",
    "    p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude')\n",
    "    time = np.linspace(0, len(audio)/srate, num=len(audio))\n",
    "    p.line(time, audio)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the frequency-domain representation of an audio file:\n",
    "\n",
    "def plot_freq_domain(audio, srate):\n",
    "    f, t, s = scipy.signal.spectrogram(audio, srate)\n",
    "    s = 10 * np.log10(s + 1e-40)\n",
    "    p = figure(plot_width=800, plot_height=400, x_axis_label='Time (s)', y_axis_label='Frequency (Hz)')\n",
    "    p.image(image=[s], x=0, y=0, dw=t[-1], dh=f[-1], palette=\"Viridis256\", level=\"image\")\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba547a",
   "metadata": {},
   "source": [
    "# Audio Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell denoises an audio file:\n",
    "\n",
    "def denoise(\n",
    "    audio, \n",
    "    noise, \n",
    "    n_grad_freq   = 3,\n",
    "    n_grad_time   = 4,\n",
    "    n_fft         = 2048,\n",
    "    win_length    = 2048,\n",
    "    hop_length    = 512,\n",
    "    n_std_thresh  = 1,\n",
    "    prop_decrease = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    Remove noise from audio based upon a clip containing only noise\n",
    "\n",
    "    Args:\n",
    "        audio (array)        : audio to denoise\n",
    "        noise (array)        : noise sample\n",
    "        n_grad_freq (int)    : how many frequency channels to smooth over with the mask.\n",
    "        n_grad_time (int)    : how many time channels to smooth over with the mask.\n",
    "        n_fft (int)          : number audio of frames between STFT columns.\n",
    "        win_length (int)     : Each frame of audio is windowed by `window()`. The window will be of length `win_length` and then padded with zeros to match `n_fft`..\n",
    "        hop_length (int)     : number audio of frames between STFT columns.\n",
    "        n_std_thresh (int)   : how many standard deviations louder than the mean dB of the noise (at each frequency level) to be considered signal\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "        (array) The recovered signal with noise subtracted\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # STFT over noise:\n",
    "    \n",
    "    stft_noise    = librosa.stft(y = noise, n_fft = n_fft, hop_length = hop_length, win_length = win_length)\n",
    "    stft_noise_db = librosa.core.amplitude_to_db(np.abs(stft_noise))\n",
    "    \n",
    "    # calculate statistics over noise and noise threshold, over frequency axis:\n",
    "    \n",
    "    noise_freq_mean = np.mean(stft_noise_db, axis=1)\n",
    "    noise_freq_std  = np.std(stft_noise_db, axis=1)\n",
    "    noise_thresh    = noise_freq_mean + noise_freq_std * n_std_thresh\n",
    "    \n",
    "    # STFT over signal:\n",
    "\n",
    "    stft_audio    = librosa.stft(y = audio, n_fft = n_fft, hop_length = hop_length, win_length = win_length)\n",
    "    stft_audio_db = librosa.core.amplitude_to_db(np.abs(stft_audio))\n",
    "    \n",
    "    # calculate value to mask dB to:\n",
    "    \n",
    "    mask_gain_dB = np.min(stft_audio_db)\n",
    "    \n",
    "    # create a smoothing filter for the mask in time and frequency:\n",
    "    \n",
    "    smoothing_filter = np.outer(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.linspace(0, 1, n_grad_freq + 1, endpoint=False),\n",
    "                np.linspace(1, 0, n_grad_freq + 2),\n",
    "            ]\n",
    "        )[1:-1],\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.linspace(0, 1, n_grad_time + 1, endpoint=False),\n",
    "                np.linspace(1, 0, n_grad_time + 2),\n",
    "            ]\n",
    "        )[1:-1],\n",
    "    )\n",
    "    smoothing_filter = smoothing_filter / np.sum(smoothing_filter)\n",
    "    \n",
    "    # calculate the threshold for each frequency/time bin:\n",
    "    \n",
    "    db_thresh = np.repeat(\n",
    "        np.reshape(noise_thresh, [1, len(noise_freq_mean)]),\n",
    "        np.shape(stft_audio_db)[1],\n",
    "        axis=0,\n",
    "    ).T\n",
    "    \n",
    "    # mask if the signal is above the threshold:\n",
    "    \n",
    "    mask_audio = stft_audio_db < db_thresh\n",
    "\n",
    "    # convolve the mask with a smoothing filter:\n",
    "    \n",
    "    mask_audio = scipy.signal.fftconvolve(mask_audio, smoothing_filter, mode = \"same\")\n",
    "    mask_audio = mask_audio * prop_decrease\n",
    "\n",
    "    # mask the signal:\n",
    "    \n",
    "    stft_audio_db_masked = (stft_audio_db * (1 - mask_audio) + np.ones(np.shape(mask_gain_dB)) \n",
    "                            * mask_gain_dB * mask_audio)\n",
    "    \n",
    "    # mask real:\n",
    "    \n",
    "    audio_imag_masked = np.imag(stft_audio) * (1 - mask_audio)\n",
    "    stft_audio_amp = (librosa.core.db_to_amplitude(stft_audio_db_masked) * np.sign(stft_audio)) + (\n",
    "        1j * audio_imag_masked\n",
    "    )\n",
    "\n",
    "    # recover the signal:\n",
    "    \n",
    "    recovered_signal = librosa.istft(stft_audio_amp, hop_length = hop_length, win_length = win_length)\n",
    "\n",
    "    return recovered_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7064a",
   "metadata": {},
   "source": [
    "# Audio Cross-Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell computes and visualizes the cross-correlation (or autocorrelation) between two signals:\n",
    "\n",
    "def correlation(audio_probe, audio_query, srate):\n",
    "    \n",
    "    xcorrelation = np.correlate(audio_probe, audio_query, mode = 'full')\n",
    "    \n",
    "    p = figure(plot_width=800, plot_height=200, x_axis_label='Delay (s)', y_axis_label='Cross-Correlation')\n",
    "    delay = np.linspace(0, len(audio_query)/srate, num=len(audio_query))\n",
    "    p.line(delay, xcorrelation)\n",
    "    show(p)\n",
    "    \n",
    "    return xcorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca327f78",
   "metadata": {},
   "source": [
    "# Audio Monophonic Pitch Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell implements different monophonic pitch detection methods:\n",
    "\n",
    "def pitch_zero_crossings(frame, srate): \n",
    "    \n",
    "    zero_indices   = np.nonzero((frame[1:] >= 0) & (frame[:-1] < 0))[0]\n",
    "    pitch_estimate = (srate / np.mean(np.diff(indices)))\n",
    "    \n",
    "    return pitch_estimate \n",
    "\n",
    "def pitch_fft(frame, srate): \n",
    "    \n",
    "    mag            = np.abs(np.fft.fft(frame))\n",
    "    mag            = mag[0:int(len(mag)/2)]\n",
    "    pitch_estimate = np.argmax(mag) * (srate / len(frame))\n",
    "    \n",
    "    return pitch_estimate \n",
    "\n",
    "def pitch_autocorrelation(frame, srate):\n",
    "    \n",
    "    xcorrelation   = np.correlate(frame, frame, mode = 'full')\n",
    "    derivative     = np.diff(xcorrelation[:int(len(xcorrelation)/2)+2])\n",
    "    peak_indices   = np.nonzero((derivative[:-1] > 0) & (derivative[1:] <= 0))[0] + 1\n",
    "    peak_values    = xcorrelation[peak_indices]\n",
    "    peak_indices_sorted = peak_indices[np.argsort(peak_values)[-2:]]\n",
    "    \n",
    "    return srate/(peak_indices_sorted[1]-peak_indices_sorted[0])\n",
    "\n",
    "def pitch_track(signal, hopSize, winSize, extractor, srate): \n",
    "    \n",
    "    offsets = np.arange(0, len(signal), hopSize)\n",
    "    pitch_track = np.zeros(len(offsets))\n",
    "    amp_track = np.zeros(len(offsets))\n",
    "    \n",
    "    for (m, o) in enumerate(offsets): \n",
    "        frame = signal[o:o+winSize] \n",
    "        pitch_track[m] = extractor(frame, srate)\n",
    "        amp_track[m] = np.sqrt(np.mean(np.square(frame)))  \n",
    "\n",
    "        if (pitch_track[m] > 1500): \n",
    "            pitch_track[m] = 0 \n",
    "    \n",
    "    return (amp_track, pitch_track)\n",
    "\n",
    "def sonify(amp_track, pitch_track, srate, hop_size):\n",
    "\n",
    "    times = np.arange(0.0, float(hop_size * len(pitch_track)) / srate,\n",
    "                      float(hop_size) / srate)\n",
    "\n",
    "    # sample locations in time (seconds)                                                      \n",
    "    sample_times = np.linspace(0, np.max(times), int(np.max(times)*srate-1))\n",
    "\n",
    "    freq_interpolator = scipy.interpolate.interp1d(times,pitch_track)\n",
    "    amp_interpolator = scipy.interpolate.interp1d(times,amp_track)\n",
    "                                                                \n",
    "    sample_freqs = freq_interpolator(sample_times)\n",
    "    sample_amps  = amp_interpolator(sample_times)\n",
    "\n",
    "    audio = np.zeros(len(sample_times));\n",
    "    T = 1.0 / srate\n",
    "    phase = 0.0\n",
    "    \n",
    "    for i in range(1, len(audio)):\n",
    "        audio[i] = sample_amps[i] * np.sin(phase)\n",
    "        phase = phase + (2*np.pi*T*sample_freqs[i])\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f2744",
   "metadata": {},
   "source": [
    "# Audio Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd573b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell computes the dynamic time warping between two audio signals\n",
    "\n",
    "def dtw_table(x, y, distance = None):\n",
    "    \n",
    "    if distance is None:\n",
    "        distance = scipy.spatial.distance.euclidean\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    table = np.zeros((nx+1, ny+1))\n",
    "    \n",
    "    # compute left column separately, i.e. j=0.\n",
    "    table[1:, 0] = np.inf\n",
    "        \n",
    "    # compute top row separately, i.e. i=0.\n",
    "    table[0, 1:] = np.inf\n",
    "        \n",
    "    # Fill in the rest.\n",
    "    for i in range(1, nx+1):\n",
    "        for j in range(1, ny+1):\n",
    "            d = distance(x[i-1], y[j-1])\n",
    "            table[i, j] = d + min(table[i-1, j], table[i, j-1], table[i-1, j-1])\n",
    "    return table\n",
    "\n",
    "# dtw table traceback function:\n",
    "\n",
    "def dtw_path(x, y, table):\n",
    "    \n",
    "    i = len(x)\n",
    "    j = len(y)\n",
    "    path = [(i, j)]\n",
    "    while i > 0 or j > 0:\n",
    "        minval = np.inf\n",
    "        if table[i-1][j-1] < minval:\n",
    "            minval = table[i-1, j-1]\n",
    "            step = (i-1, j-1)\n",
    "        if table[i-1, j] < minval:\n",
    "            minval = table[i-1, j]\n",
    "            step = (i-1, j)\n",
    "        if table[i][j-1] < minval:\n",
    "            minval = table[i, j-1]\n",
    "            step = (i, j-1)\n",
    "        path.insert(0, step)\n",
    "        i, j = step\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# plot dtw table and path, along with signals:\n",
    "\n",
    "def plot_dtw(table, path, signal1, signal2):\n",
    "    \n",
    "    %matplotlib widget\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "\n",
    "    # Bottom right plot.\n",
    "    ax1 = plt.axes([0.2, 0, 0.8, 0.2])\n",
    "    ax1.imshow(signal1, origin = 'upper', aspect = 'auto', cmap = 'coolwarm')\n",
    "    ax1.set_xlabel('Signal 1')\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_ylim(10)\n",
    "\n",
    "    # Top left plot.\n",
    "    ax2 = plt.axes([0, 0.2, 0.20, 0.8])\n",
    "    ax2.imshow(signal2.T, origin = 'lower', aspect = 'auto', cmap = 'coolwarm')\n",
    "    ax2.set_ylabel('Signal 2')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_ylim(1)\n",
    "\n",
    "    # Top right plot.\n",
    "    ax3 = plt.axes([0.2, 0.2, 0.8, 0.8], sharex = ax1, sharey = ax2)\n",
    "    ax3.imshow(table.T, aspect = 'auto', origin = 'upper', interpolation = 'nearest', cmap = 'gray')\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "\n",
    "    # Path.\n",
    "    ax3.plot(path[:,0], path[:,1], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bac0fd",
   "metadata": {},
   "source": [
    "# Audio Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell trims a denoised call file based on a amplitude/pitch track version:\n",
    "\n",
    "def trim(audio, amplitude_track, srate, binSize = 6, hopSize = 256, threshold = 2.5, sensitivity = 5):\n",
    "\n",
    "    # get noise at beginning and end of audio clip: \n",
    "    \n",
    "    begin_noise_level = np.mean(amplitude_track[:int(len(amplitude_track)/binSize)])\n",
    "    end_noise_level   = np.mean(amplitude_track[-int(len(amplitude_track)/binSize):])\n",
    "        \n",
    "    index_first = None\n",
    "    index_last  = None\n",
    "    \n",
    "    # if signal > threshold * begin_noise_level many times in a row, trim here (from track start):\n",
    "    \n",
    "    for i in range(len(amplitude_track)):\n",
    "        if amplitude_track[i] > threshold * begin_noise_level:\n",
    "            if all(amplitude_track[j] > threshold * begin_noise_level for j in range(i, i + sensitivity + 1)):\n",
    "                index_first = i\n",
    "                break\n",
    "    \n",
    "    # if signal > threshold * end_noise_level many times in a row, trim here (from track end):\n",
    "\n",
    "    for i in range(len(amplitude_track)):\n",
    "        if amplitude_track[i] > threshold * end_noise_level:\n",
    "            if all(amplitude_track[j] > threshold * end_noise_level for j in range(i - sensitivity + 2, i + 1)):\n",
    "                index_last = i\n",
    "    \n",
    "    return audio[index_first * hopSize:(index_last + 10) * hopSize]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a6897",
   "metadata": {},
   "source": [
    "# Audio Probe Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a probe sequence and a query sequence, and locates the probe in the query:\n",
    "\n",
    "def probe_localization_xcorrelation_time(probe_audio, query_audio, winSize, srate, threshold, mode):\n",
    "    \n",
    "    # get correlation between probe and query, and probe and probe:\n",
    "    \n",
    "    correlation_pq = np.correlate(probe_audio, query_audio, mode = 'same')[::-1]\n",
    "    correlation_pp = np.correlate(probe_audio, probe_audio, mode = 'same')\n",
    "    \n",
    "    # mode:\n",
    "    \n",
    "    if mode == 'log':\n",
    "        \n",
    "        correlation_pq = np.log(correlation_pq)\n",
    "        correlation_pp = np.log(correlation_pp)\n",
    "    \n",
    "    # define localization threshold based on probe-probe correlation:\n",
    "    \n",
    "    thresh = threshold * max(correlation_pp)\n",
    "\n",
    "    offsets = np.arange(0, len(correlation_pq), winSize)\n",
    "    amp = np.zeros(len(offsets))\n",
    "    \n",
    "    for (m, o) in enumerate(offsets): \n",
    "        frame = correlation_pq[o:o+winSize] \n",
    "        amp[m] = np.max(np.abs(frame))\n",
    "    \n",
    "    localizations = np.array([i for i in range(1, len(amp) - 1) \n",
    "                              if (amp[i] > amp[i - 1] and amp[i] > amp[i + 1] and amp[i] > thresh)])\n",
    "    \n",
    "    return amp, localizations * winSize / srate\n",
    "\n",
    "def probe_localization_xcorrelation_stft(probe_audio, query_audio, winSize, srate, threshold):\n",
    "    \n",
    "    # compute STFT of probe and query audio:\n",
    "    \n",
    "    stft_probe = librosa.stft(probe_audio, n_fft=winSize)\n",
    "    stft_query = librosa.stft(query_audio, n_fft=winSize)\n",
    "\n",
    "    # calculate magnitude spectra:\n",
    "    \n",
    "    mag_probe = np.abs(stft_probe)\n",
    "    mag_query = np.abs(stft_query)\n",
    "\n",
    "    # compute cross-correlation in frequency domain:\n",
    "    \n",
    "    correlation_pq = np.abs(np.correlate(mag_probe.flatten(), mag_query.flatten(), mode='same'))\n",
    "\n",
    "    # compute probe-probe correlation:\n",
    "    \n",
    "    correlation_pp = np.correlate(mag_probe.flatten(), mag_probe.flatten(), mode='same')\n",
    "\n",
    "    # define localization threshold based on probe-probe correlation:\n",
    "    \n",
    "    thresh = threshold * np.max(correlation_pp)\n",
    "\n",
    "    # find peaks in the correlation:\n",
    "    \n",
    "    offsets = np.arange(0, len(correlation_pq), winSize)\n",
    "    amp = np.zeros(len(offsets))\n",
    "    \n",
    "    for (m, o) in enumerate(offsets): \n",
    "        frame = correlation_pq[o:o+winSize] \n",
    "        amp[m] = np.max(np.abs(frame))\n",
    "    \n",
    "    localizations = np.array([i for i in range(1, len(amp) - 1) \n",
    "                              if (amp[i] > amp[i - 1] and amp[i] > amp[i + 1] and amp[i] > thresh)])\n",
    "    \n",
    "    return amp, localizations * winSize / (2 * srate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1fea5",
   "metadata": {},
   "source": [
    "# Audio Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell measures the similarity between two calls, using a variety of methods:\n",
    "\n",
    "# cross-correlation peak max:\n",
    "\n",
    "def similarity_cross_correlation(audio_1, audio_2):\n",
    "    \n",
    "    correlation = np.correlate(audio_1, audio_2, mode = \"same\")\n",
    "    \n",
    "    return max(correlation)\n",
    "\n",
    "# dtw distance:\n",
    "\n",
    "def similarity_dynamic_time_warping(audio_1, audio_2):\n",
    "    \n",
    "    distance, path = fastdtw(audio_1, audio_2)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# mfcc distance:\n",
    "\n",
    "def similarity_mfcc(audio_1, audio_2, srate):\n",
    "    \n",
    "    mfccs_1 = librosa.feature.mfcc(y = audio_1, sr = srate)\n",
    "    mfccs_2 = librosa.feature.mfcc(y = audio_2, sr = srate)\n",
    "    \n",
    "    distance = np.linalg.norm(mfccs_1 - mfccs_2)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a28d80",
   "metadata": {},
   "source": [
    "# Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1970a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_file, srate):\n",
    "\n",
    "    # initialize features dictionary:\n",
    "    \n",
    "    features_dict = {}\n",
    "    \n",
    "    # diy features:\n",
    "    \n",
    "    features_dict['length']    = len(audio_file)/srate\n",
    "    #features_dict['frequency'] = np.abs(np.fft.fftfreq(len(audio_file), 1 / srate)[np.argmax(np.abs(np.fft.fft(audio_file)))])\n",
    "    \n",
    "    # spectral features:\n",
    "    \n",
    "    spectral_centroid  = librosa.feature.spectral_centroid(y = audio_file, sr = srate)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y = audio_file, sr = srate)\n",
    "    spectral_contrast  = librosa.feature.spectral_contrast(y = audio_file, sr = srate)\n",
    "    spectral_flatness  = librosa.feature.spectral_flatness(y = audio_file)\n",
    "    spectral_rolloff   = librosa.feature.spectral_rolloff(y = audio_file, sr = srate)\n",
    "    \n",
    "    features_dict['spectral_centroid_mean']  = spectral_centroid.mean()\n",
    "    features_dict['spectral_bandwidth_mean'] = spectral_bandwidth.mean()\n",
    "    features_dict['spectral_contrast_mean']  = spectral_contrast.mean()\n",
    "    features_dict['spectral_flatness_mean']  = spectral_flatness.mean()\n",
    "    features_dict['spectral_rolloff_mean']   = spectral_rolloff.mean()\n",
    "     \n",
    "    features_dict['spectral_centroid_std']  = spectral_centroid.std()\n",
    "    features_dict['spectral_bandwidth_std'] = spectral_bandwidth.std()\n",
    "    features_dict['spectral_contrast_std']  = spectral_contrast.std()\n",
    "    features_dict['spectral_flatness_std']  = spectral_flatness.std()\n",
    "    features_dict['spectral_rolloff_std']   = spectral_rolloff.std()\n",
    "\n",
    "    # temporal features:\n",
    "    \n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y = audio_file)\n",
    "    rmse = librosa.feature.rms(y = audio_file)\n",
    "    \n",
    "    features_dict['zero_crossing_rate_mean'] = zero_crossing_rate.mean()\n",
    "    features_dict['rmse_mean'] = rmse.mean()\n",
    "\n",
    "    features_dict['zero_crossing_rate_std'] = zero_crossing_rate.std()\n",
    "    features_dict['rmse_std'] = rmse.std()\n",
    "    \n",
    "    # MFCCs:\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y = audio_file, sr = srate)\n",
    "    for i in range(mfccs.shape[0]):\n",
    "        features_dict[f'mfcc_{i+1}'] = mfccs[i].mean()\n",
    "\n",
    "    # Additional features\n",
    "    chroma = librosa.feature.chroma_stft(y = audio_file, sr = srate)\n",
    "    #tempogram = librosa.feature.tempogram(y = audio_file, sr = srate)\n",
    "    tonnetz = librosa.feature.tonnetz(y = audio_file, sr = srate)\n",
    "    \n",
    "    for i in range(chroma.shape[0]):\n",
    "        features_dict[f'chroma_{i+1}'] = chroma[i].mean()\n",
    "        \n",
    "    #for i in range(tempogram.shape[0]):\n",
    "    #    features_dict[f'tempogram_{i+1}'] = tempogram[i].mean()\n",
    "        \n",
    "    for i in range(tonnetz.shape[0]):\n",
    "        features_dict[f'tonnetz_{i+1}'] = tonnetz[i].mean()\n",
    "\n",
    "    return features_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d159a8",
   "metadata": {},
   "source": [
    "# Audio Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes an array of feature vectors, and clusters them:\n",
    "\n",
    "def cluster_adaptive_kmeans(data, max_clusters = 50, tolerance = 0.01):\n",
    "    \n",
    "    kmeans = sklearn.cluster.Kmeans(n_clusters = 10, random_state = 0).fit(data)\n",
    "    prev_inertia = kmeans.inertia_\n",
    "    \n",
    "    for n_clusters in range(11, max_clusters + 1):\n",
    "        \n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = 0).fit(data)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        if (prev_inertia - inertia) / prev_inertia < tolerance:\n",
    "            break\n",
    "        \n",
    "        prev_inertia = inertia\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def cluster_dbscan(data, eps = 0.5, min_samples = 5):\n",
    "    \n",
    "    dbscan = sklearn.cluster.DBSCAN(eps = eps, min_samples = min_samples)    \n",
    "    dbscan.fit(data)\n",
    "    labels = dbscan.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    \n",
    "    return labels, n_clusters_, n_noise_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce82988",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ca728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load two calls:\n",
    "\n",
    "(whale_1, srate) = load_audio('whale1.wav')\n",
    "(whale_2, srate) = load_audio('whale2.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 (original):\n",
    "\n",
    "plot_time_domain(whale_1, srate)\n",
    "plot_freq_domain(whale_1, srate)\n",
    "ipd.Audio(whale_1, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ea4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 (denoised):\n",
    "\n",
    "whale_1_denoised = denoise(whale_1, whale_1[:int(len(whale_1)/10)], n_std_thresh = 1.5)\n",
    "plot_time_domain(whale_1_denoised, srate)\n",
    "plot_freq_domain(whale_1_denoised, srate)\n",
    "ipd.Audio(whale_1_denoised, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ce786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 (original):\n",
    "\n",
    "plot_time_domain(whale_2, srate)\n",
    "plot_freq_domain(whale_2, srate)\n",
    "ipd.Audio(whale_2, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8443ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 (denoised):\n",
    "\n",
    "whale_2_denoised = denoise(whale_2, whale_2[:int(len(whale_2)/10)], n_std_thresh = 1)\n",
    "plot_time_domain(whale_2_denoised, srate)\n",
    "plot_freq_domain(whale_2_denoised, srate)\n",
    "ipd.Audio(whale_2_denoised, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 monophonic pitch detection:\n",
    "\n",
    "whale_1_amp, whale_1_pitch  = pitch_track(whale_1_denoised, 256, 512, pitch_autocorrelation, 44100)\n",
    "whale_1_pitch_filtered      = signal.medfilt(whale_1_pitch, kernel_size=15)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Pitch (Extracted)')\n",
    "time = np.linspace(0, len(whale_1)/srate, num=len(whale_1_pitch_filtered))\n",
    "p.line(time, whale_1_pitch_filtered)\n",
    "show(p)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude (Extracted)')\n",
    "time = np.linspace(0, len(whale_1)/srate, num=len(whale_1_pitch))\n",
    "p.line(time, whale_1_amp)\n",
    "show(p)\n",
    "\n",
    "whale_1_mpd                 = sonify(whale_1_amp, whale_1_pitch_filtered, srate, 256)\n",
    "ipd.Audio(whale_1_mpd, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 monophonic pitch detection:\n",
    "\n",
    "whale_2_amp, whale_2_pitch  = pitch_track(whale_2_denoised, 256, 512, pitch_autocorrelation, 44100)\n",
    "whale_2_pitch_filtered      = signal.medfilt(whale_2_pitch, kernel_size=15)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Pitch (Extracted)')\n",
    "time = np.linspace(0, len(whale_2)/srate, num=len(whale_2_pitch_filtered))\n",
    "p.line(time, whale_2_pitch_filtered)\n",
    "show(p)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude (Extracted)')\n",
    "time = np.linspace(0, len(whale_2)/srate, num=len(whale_2_pitch))\n",
    "p.line(time, whale_2_amp)\n",
    "show(p)\n",
    "\n",
    "whale_2_mpd                 = sonify(whale_2_amp, whale_2_pitch_filtered, srate, 256)\n",
    "ipd.Audio(whale_2_mpd, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls 1 and 2 alignment by DTW and visualization:\n",
    "\n",
    "whale_1_stft = librosa.feature.chroma_stft(y = whale_1_denoised, sr = srate, hop_length = 256)\n",
    "whale_2_stft = librosa.feature.chroma_stft(y = whale_2_denoised, sr = srate, hop_length = 256)\n",
    "\n",
    "distance = dtw_table(whale_1_stft.T, whale_2_stft.T, distance = scipy.spatial.distance.cosine)\n",
    "path     = dtw_path(whale_1_stft.T, whale_2_stft.T, distance)\n",
    "\n",
    "plot_dtw(distance, path, whale_1_stft, whale_2_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36770601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 denoised and trimmed:\n",
    "\n",
    "whale_1_denoised_trimmed = trim(whale_1_denoised, whale_1_amp, srate, sensitivity = 10)\n",
    "plot_time_domain(whale_1_denoised_trimmed, srate)\n",
    "plot_freq_domain(whale_1_denoised_trimmed, srate)\n",
    "ipd.Audio(whale_1_denoised_trimmed, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 denoised and trimmed:\n",
    "\n",
    "whale_2_denoised_trimmed = trim(whale_2_denoised, whale_2_amp, srate, sensitivity = 10)\n",
    "plot_time_domain(whale_2_denoised_trimmed, srate)\n",
    "plot_freq_domain(whale_2_denoised_trimmed, srate)\n",
    "ipd.Audio(whale_2_denoised_trimmed, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_1_trimmed_stft = librosa.feature.chroma_stft(y = whale_1_denoised_trimmed, sr = srate, hop_length = 256)\n",
    "whale_2_trimmed_stft = librosa.feature.chroma_stft(y = whale_2_denoised_trimmed, sr = srate, hop_length = 256)\n",
    "\n",
    "distance = dtw_table(whale_1_trimmed_stft.T, whale_2_trimmed_stft.T, distance = scipy.spatial.distance.cosine)\n",
    "path     = dtw_path(whale_1_trimmed_stft.T, whale_2_trimmed_stft.T, distance)\n",
    "\n",
    "plot_dtw(distance, path, whale_1_trimmed_stft, whale_2_trimmed_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b830cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a fake query sequence and search for probe in query:\n",
    "\n",
    "(whale_1, srate) = load_audio('whale1.wav')\n",
    "(whale_2, srate) = load_audio('whale2.wav')\n",
    "(whale_3, srate) = load_audio('whale3.wav')\n",
    "(whale_probe, srate) = load_audio('whale_probe.wav')\n",
    "\n",
    "whale_1_denoised = denoise(whale_1, whale_1[:int(len(whale_1)/10)])\n",
    "whale_2_denoised = denoise(whale_2, whale_2[:int(len(whale_2)/10)])\n",
    "whale_3_denoised = denoise(whale_3, whale_3[:int(len(whale_3)/10)])\n",
    "whale_probe_denoised = denoise(whale_probe, whale_probe[:int(len(whale_probe)/10)])\n",
    "\n",
    "whale_1_denoised_ = whale_1_denoised/np.max(np.abs(whale_1_denoised))\n",
    "whale_2_denoised_ = whale_2_denoised/np.max(np.abs(whale_2_denoised))\n",
    "whale_3_denoised_ = whale_3_denoised/np.max(np.abs(whale_3_denoised))\n",
    "whale_probe_denoised_ = whale_probe_denoised/np.max(np.abs(whale_probe_denoised))\n",
    "\n",
    "query = np.concatenate((whale_1_denoised_, whale_2_denoised_, whale_3_denoised_))\n",
    "probe = whale_1_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b19e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_domain(whale_1_denoised, srate)\n",
    "plot_time_domain(whale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = whale_2_denoised\n",
    "stft_probe = librosa.stft(probe)\n",
    "stft_query = librosa.stft(query)\n",
    "mag_probe = np.abs(stft_probe)\n",
    "mag_query = np.abs(stft_query)\n",
    "correlation_pq = np.abs(np.correlate(mag_probe.flatten(), mag_query.flatten(), mode='full'))\n",
    "correlation_pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_domain(correlation_pq, srate) # whale_1_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_domain(correlation_pq, srate) # whale_2_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ef2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_domain(correlation_pq, srate) # whale_3_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization = probe_localization_xcorrelation_stft(probe, query, 22050, 44100, 0.1)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude')\n",
    "p.line(np.linspace(0, len(query)/44100, len(query)), query)\n",
    "show(p)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Delay (s)', y_axis_label='Cross-Correlation')\n",
    "p.line(np.linspace(0, int(len(localization[0]) * 22050 / (2 * 44100) ), len(localization[0])), localization[0])\n",
    "show(p)\n",
    "\n",
    "print(\"Calls located at: \", *localization[1], \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Delay (s)', y_axis_label='Cross-Correlation')\n",
    "p.line(np.linspace(0, int(len(localization[0]) * 22050 / (2 * 44100) ), len(localization[0])), localization[0]**5)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad34b44",
   "metadata": {},
   "source": [
    "## Call Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34362e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_according_to_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05026182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve audio file names, pods, call types:\n",
    "\n",
    "files = os.listdir('c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data')\n",
    "\n",
    "pods  = np.array([string.split('-')[0] for string in files])\n",
    "types = np.array([string.split('-')[1] for string in files])\n",
    "\n",
    "pods_unique  = np.unique(pods)\n",
    "types_unique = np.unique(types)\n",
    "\n",
    "files_according_to_pod  = {x.split('-')[0]: [f for f in files if f.startswith(x.split('-')[0])] for x in files}\n",
    "files_according_to_type = {x.split('-')[1]: [f for f in files if f.split('-')[1] == x.split('-')[1]] for x in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files, denoise, and trim:\n",
    "\n",
    "for i in files:\n",
    "    \n",
    "    try: \n",
    "\n",
    "        # load audio:\n",
    "\n",
    "        audio = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/call-catalog/orig/wav/\"+i)\n",
    "\n",
    "        # denoise:\n",
    "\n",
    "        audio_denoised = denoise(audio[0], audio[0][:int(len(audio[0])/10)])\n",
    "\n",
    "        # trim:\n",
    "\n",
    "        audio_denoised_amp, audio_denoised_pitch  = pitch_track(audio_denoised, 256, 512, pitch_autocorrelation, audio[1])\n",
    "        audio_denoised_trimmed = trim(audio_denoised, audio_denoised_amp, audio[1], sensitivity = 10)\n",
    "\n",
    "        # output:\n",
    "\n",
    "        sf.write(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+i, audio_denoised_trimmed, audio[1])\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e168ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_file, srate):\n",
    "\n",
    "    # initialize features dictionary:\n",
    "    \n",
    "    features_dict = {}\n",
    "    \n",
    "    # diy features:\n",
    "    \n",
    "    features_dict['length']    = len(audio_file)/srate\n",
    "\n",
    "    # temporal features:\n",
    "    \n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y = audio_file)\n",
    "    rmse = librosa.feature.rms(y = audio_file)\n",
    "    \n",
    "    features_dict['zero_crossing_rate_mean'] = zero_crossing_rate.mean()\n",
    "    features_dict['rmse_mean'] = rmse.mean()\n",
    "\n",
    "    features_dict['zero_crossing_rate_std'] = zero_crossing_rate.std()\n",
    "    features_dict['rmse_std'] = rmse.std()\n",
    "    \n",
    "    # MFCCs:\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y = audio_file, sr = srate)\n",
    "    for i in range(mfccs.shape[0]):\n",
    "        features_dict[f'mfcc_{i+1}'] = mfccs[i].mean()\n",
    "\n",
    "    # Additional features\n",
    "    chroma = librosa.feature.chroma_stft(y = audio_file, sr = srate)\n",
    "    #tempogram = librosa.feature.tempogram(y = audio_file, sr = srate)\n",
    "    tonnetz = librosa.feature.tonnetz(y = audio_file, sr = srate)\n",
    "    \n",
    "    for i in range(chroma.shape[0]):\n",
    "        features_dict[f'chroma_{i+1}'] = chroma[i].mean()\n",
    "        \n",
    "    #for i in range(tempogram.shape[0]):\n",
    "    #    features_dict[f'tempogram_{i+1}'] = tempogram[i].mean()\n",
    "        \n",
    "    for i in range(tonnetz.shape[0]):\n",
    "        features_dict[f'tonnetz_{i+1}'] = tonnetz[i].mean()\n",
    "\n",
    "    return features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global call variances across pods:\n",
    "\n",
    "features = []\n",
    "\n",
    "for key, values in files_according_to_type.items():\n",
    "            \n",
    "    for filename in values:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            audio = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+filename)\n",
    "            stuff = list(extract_audio_features(audio[0], audio[1]).values())\n",
    "            features.append([key, *stuff])\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(features, columns = [\"key\", *list(extract_audio_features(audio[0], audio[1]).keys())])\n",
    "data.to_csv(\"./data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct PCA and look at variance of first two components:\n",
    "\n",
    "labels_  = [sublist[0] for sublist in features] \n",
    "feature_ = [sublist[1:] for sublist in features] \n",
    "colors = ['#{:02x}{:02x}{:02x}'.format(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for _ in range(55)]\n",
    "y        = [{label: idx for idx, label in enumerate(set(labels_))}[string] for string in labels_]\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "features_PCA = pca.fit_transform(feature_)\n",
    "\n",
    "# plot PCA:\n",
    "\n",
    "source_features_PCA = ColumnDataSource(data=dict(\n",
    "    x = features_PCA[:, 0],\n",
    "    y = features_PCA[:, 1],\n",
    "    color = [colors[i] for i in y]\n",
    "    #label = [[\"instrumental\", \"hiphop\", \"rock\", \"folk\"][i] for i in y]\n",
    "))\n",
    "\n",
    "p = figure(title=\"PCA of meanMFCC\", plot_width = 500, plot_height = 500)\n",
    "p.scatter(x = 'x', y = 'y', size = 8,  color = 'color', source = source_features_PCA)\n",
    "p.xaxis.axis_label = 'Principal Component 1'\n",
    "p.yaxis.axis_label = 'Principal Component 2'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c7330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features_PCA, columns = [\"PCA1\", \"PCA2\"])\n",
    "df['Label'] = labels_\n",
    "\n",
    "# compute variance across each axis:\n",
    "\n",
    "df_avg = df.groupby('Label').mean()\n",
    "df_std = df.groupby('Label', as_index = False).std()\n",
    "\n",
    "# arrange data according to weighted sum over variances:\n",
    "\n",
    "weights = {'PCA1': pca.explained_variance_ratio_[0], 'PCA2': pca.explained_variance_ratio_[1]}\n",
    "weighted_sum = df_std[['PCA1', 'PCA2']].mul(weights).sum(axis=1)\n",
    "df_std['Weighted_Sum'] = weighted_sum\n",
    "df_std_sorted = df_std.sort_values(by = 'Weighted_Sum', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnDataSource\n",
    "\n",
    "labels = df_std_sorted[\"Label\"]\n",
    "weighted_sums = df_std_sorted[\"Weighted_Sum\"]\n",
    "\n",
    "source = ColumnDataSource(data = dict(labels = labels, weighted_sums = weighted_sums))\n",
    "\n",
    "# Create a figure\n",
    "p = figure(x_range = labels, plot_height = 400, plot_width = 800, toolbar_location=None, tools=\"\")\n",
    "p.circle(x='labels', y='weighted_sums', size = 10, color=\"navy\", source=source)\n",
    "p.xaxis.major_label_orientation = np.pi / 4\n",
    "p.xaxis.axis_label = \"Call Type\"\n",
    "p.yaxis.axis_label = \"Weighted 2-Component PCA Standard Deviation\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6fc20",
   "metadata": {},
   "source": [
    "## N-Gram Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each call-type, select one probe:\n",
    "\n",
    "probes = []\n",
    "\n",
    "for key, values in files_according_to_type.items():\n",
    "    \n",
    "    probes.append([key, load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"\n",
    "                                  +values[0])[0]])\n",
    "\n",
    "probes = [[\"honk\", whale_1_denoised], [\"N04\", whale_2_denoised], [\"N09\", whale_3_denoised]]\n",
    "    \n",
    "# load big audio track:\n",
    "\n",
    "query = np.concatenate((whale_1_denoised_, whale_2_denoised_, whale_3_denoised_))\n",
    "#query = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/432A.wav\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect timestamps per probe:\n",
    "\n",
    "timestamps = []\n",
    "\n",
    "for probe in probes:\n",
    "    \n",
    "    print(probe[0])\n",
    "    times  = probe_localization_xcorrelation_stft(probe[1], query, 22050, 44100, 0.25)[1]\n",
    "    stamps = [[probe[0], value] for value in times]\n",
    "    \n",
    "    for stamp in stamps:\n",
    "        \n",
    "        timestamps.append(stamp)\n",
    "    \n",
    "# order by timestamp:\n",
    "\n",
    "timestamps_sorted = sorted(timestamps, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa32091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary of n-grams and increase count:\n",
    "\n",
    "ngrams    = {}\n",
    "threshold = 10 \n",
    "\n",
    "for i in range(0, timestamps_sorted - 1):\n",
    "    \n",
    "    label = ','.join(sorted([timestamps_sorted[i][0], timestamps_sorted[i+1][0]]))\n",
    "    \n",
    "    if timestamps_sorted[i+1][1] - timestamps_sorted[i][1] < 10:\n",
    "        \n",
    "        if label in ngrams:\n",
    "            ngrams[label] += 1\n",
    "        else:\n",
    "            ngrams[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ngram frequencies:\n",
    "\n",
    "ngrams_counts = list(ngrams.items())\n",
    "ngrams_counts_sorted = sorted(ngrams_counts, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "labels = [pair[0] for pair in ngrams_counts_sorted]\n",
    "counts = [pair[1] for pair in ngrams_counts_sorted]\n",
    "\n",
    "source = ColumnDataSource(data = dict(labels = labels, counts = counts))\n",
    "\n",
    "# Create a figure\n",
    "p = figure(x_range = labels, plot_height = 400, plot_width = 800, toolbar_location=None, tools=\"\")\n",
    "p.circle(x='labels', y='counts', size = 10, color=\"navy\", source=source)\n",
    "p.xaxis.major_label_orientation = np.pi / 4\n",
    "p.xaxis.axis_label = \"Call N-Gram\"\n",
    "p.yaxis.axis_label = \"Count\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e0990",
   "metadata": {},
   "source": [
    "## X-Correlation and DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_N01 = files_according_to_type['N01']\n",
    "files_N09 = files_according_to_type['N09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all pairwise similarities for each call type:\n",
    "\n",
    "similarities_xcorrelation_N01  = []\n",
    "similarities_dtw_N01           = []\n",
    "similarities_mfcc_N01          = []\n",
    "\n",
    "for i in range(0, len(files_N01)):\n",
    "    \n",
    "    audio_1 = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+files_N01[i])\n",
    "    \n",
    "    for j in range(i+1, len(files_N01)):\n",
    "        \n",
    "        audio_2 = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+files_N01[j])\n",
    "    \n",
    "        similarities_xcorrelation_N01.append(similarity_cross_correlation(audio_1[0], audio_2[0]))\n",
    "        similarities_dtw_N01.append(similarity_dynamic_time_warping(audio_1[0], audio_2[0]))\n",
    "        #similarities_mfcc_N01.append(similarity_mfcc(audio_1[0], audio_2[0], audio_1[1]))\n",
    "\n",
    "similarities_xcorrelation_N09  = []\n",
    "similarities_dtw_N09           = []\n",
    "similarities_mfcc_N09          = []\n",
    "\n",
    "for i in range(0, len(files_N09)):\n",
    "    \n",
    "    audio_1 = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+files_N09[i])\n",
    "    \n",
    "    for j in range(i+1, len(files_N09)):\n",
    "        \n",
    "        audio_2 = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+files_N09[j])\n",
    "    \n",
    "        similarities_xcorrelation_N09.append(similarity_cross_correlation(audio_1[0], audio_2[0]))\n",
    "        similarities_dtw_N09.append(similarity_dynamic_time_warping(audio_1[0], audio_2[0]))\n",
    "        #similarities_mfcc_N01.append(similarity_mfcc(audio_1[0], audio_2[0], audio_1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "N01 = [similarities_xcorrelation_N01, similarities_dtw_N01]\n",
    "N09 = [similarities_xcorrelation_N09, similarities_dtw_N09]\n",
    "\n",
    "ALL = [N01, N09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data:\n",
    "\n",
    "N01_XCOR = N01[0]\n",
    "N01_DTW = N01[1]\n",
    "N09_XCOR = N09[0]\n",
    "N09_DTW = N09[1]\n",
    "\n",
    "# Creating boxplot:\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.boxplot([N01_XCOR, N09_XCOR], labels=['N01', 'N09'])\n",
    "plt.xlabel('Call Type')\n",
    "plt.ylabel('Cross-Correlation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.boxplot([N01_DTW, N09_DTW], labels=['N01', 'N09'])\n",
    "plt.xlabel('Call Type')\n",
    "plt.ylabel('DTW')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5785d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Perform Kolmogorov-Smirnov two-sample test\n",
    "statistic, p_value = stats.ks_2samp(N01_XCOR, N09_XCOR)\n",
    "\n",
    "# Print the results\n",
    "print(\"K-S statistic:\", statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: distributions are different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: distributions are the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform Kolmogorov-Smirnov two-sample test\n",
    "statistic, p_value = stats.ks_2samp(N01_DTW, N09_DTW)\n",
    "\n",
    "# Print the results\n",
    "print(\"K-S statistic:\", statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: distributions are different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: distributions are the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b83aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
